{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Code referenced from  https://github.com/sahanbull/context-agnostic-engagement/blob/0472e76c6bd00d686b235d844e2fb4d71649400c/context_agnostic_engagement/feature_extraction/_api_utils.py#L47"
      ],
      "metadata": {
        "id": "BMWPdmr99Zjd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JG3_aLgeIOAL",
        "outputId": "0df4c97f-8f02-4a52-d058-4c23f9362b48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-terrier\n",
            "  Downloading python-terrier-0.8.1.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 3.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from python-terrier) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from python-terrier) (1.3.5)\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from python-terrier) (4.64.1)\n",
            "Collecting pyjnius~=1.3.0\n",
            "  Downloading pyjnius-1.3.0-cp37-cp37m-manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 24.0 MB/s \n",
            "\u001b[?25hCollecting matchpy\n",
            "  Downloading matchpy-0.5.5-py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 7.4 MB/s \n",
            "\u001b[?25hCollecting sklearn\n",
            "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
            "Collecting deprecation\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting chest\n",
            "  Downloading chest-0.2.3.tar.gz (9.6 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from python-terrier) (1.7.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from python-terrier) (2.23.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from python-terrier) (1.1.0)\n",
            "Collecting nptyping==1.4.4\n",
            "  Downloading nptyping-1.4.4-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: more_itertools in /usr/local/lib/python3.7/dist-packages (from python-terrier) (8.14.0)\n",
            "Collecting ir_datasets>=0.3.2\n",
            "  Downloading ir_datasets-0.5.3-py3-none-any.whl (303 kB)\n",
            "\u001b[K     |████████████████████████████████| 303 kB 55.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from python-terrier) (2.11.3)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (from python-terrier) (0.12.2)\n",
            "Collecting ir_measures>=0.2.0\n",
            "  Downloading ir_measures-0.3.1.tar.gz (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from python-terrier) (0.3.5.1)\n",
            "Collecting typish>=1.7.0\n",
            "  Downloading typish-1.9.3-py3-none-any.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 2.5 MB/s \n",
            "\u001b[?25hCollecting warc3-wet>=0.2.3\n",
            "  Downloading warc3_wet-0.2.3-py3-none-any.whl (13 kB)\n",
            "Collecting ijson>=3.1.3\n",
            "  Downloading ijson-3.1.4-cp37-cp37m-manylinux2010_x86_64.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 58.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from ir_datasets>=0.3.2->python-terrier) (4.6.3)\n",
            "Collecting zlib-state>=0.1.3\n",
            "  Downloading zlib_state-0.1.5-cp37-cp37m-manylinux2010_x86_64.whl (72 kB)\n",
            "\u001b[K     |████████████████████████████████| 72 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.7/dist-packages (from ir_datasets>=0.3.2->python-terrier) (4.9.1)\n",
            "Collecting lz4>=3.1.1\n",
            "  Downloading lz4-4.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 56.0 MB/s \n",
            "\u001b[?25hCollecting warc3-wet-clueweb09>=0.2.5\n",
            "  Downloading warc3-wet-clueweb09-0.2.5.tar.gz (17 kB)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.7/dist-packages (from ir_datasets>=0.3.2->python-terrier) (6.0)\n",
            "Collecting trec-car-tools>=2.5.4\n",
            "  Downloading trec_car_tools-2.6-py3-none-any.whl (8.4 kB)\n",
            "Collecting unlzw3>=0.2.1\n",
            "  Downloading unlzw3-0.2.1.tar.gz (5.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyautocorpus>=0.1.1\n",
            "  Downloading pyautocorpus-0.1.8-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 72.8 MB/s \n",
            "\u001b[?25hCollecting pytrec-eval-terrier>=0.5.2\n",
            "  Downloading pytrec_eval_terrier-0.5.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (285 kB)\n",
            "\u001b[K     |████████████████████████████████| 285 kB 78.1 MB/s \n",
            "\u001b[?25hCollecting cwl-eval>=1.0.10\n",
            "  Downloading cwl-eval-1.0.10.tar.gz (31 kB)\n",
            "Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from pyjnius~=1.3.0->python-terrier) (1.15.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from pyjnius~=1.3.0->python-terrier) (0.29.32)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->python-terrier) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->python-terrier) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->python-terrier) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->python-terrier) (3.0.4)\n",
            "Collecting cbor>=1.0.0\n",
            "  Downloading cbor-1.0.0.tar.gz (20 kB)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from chest->python-terrier) (1.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from deprecation->python-terrier) (21.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->python-terrier) (2.0.1)\n",
            "Collecting multiset<3.0,>=2.0\n",
            "  Downloading multiset-2.1.1-py2.py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->deprecation->python-terrier) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->python-terrier) (2022.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->python-terrier) (2.8.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->python-terrier) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->python-terrier) (3.1.0)\n",
            "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.7/dist-packages (from statsmodels->python-terrier) (0.5.2)\n",
            "Building wheels for collected packages: python-terrier, ir-measures, cwl-eval, cbor, unlzw3, warc3-wet-clueweb09, chest, sklearn, wget\n",
            "  Building wheel for python-terrier (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-terrier: filename=python_terrier-0.8.1-py3-none-any.whl size=104091 sha256=0be13bf5c705f941dcfbb6269422aeb649705ed259fd719e5b0be189f4fd5e1b\n",
            "  Stored in directory: /root/.cache/pip/wheels/09/e5/f5/2f28a11314bac89a683eb2a12aed802a7da27e4318ffbff4a8\n",
            "  Building wheel for ir-measures (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ir-measures: filename=ir_measures-0.3.1-py3-none-any.whl size=60193 sha256=8ecf2cb2047b87e936cadd4919c5cfd14c608b14b4685ebd6966fcda9d5b7f9e\n",
            "  Stored in directory: /root/.cache/pip/wheels/17/86/a6/8b9c92080cdc19346721307878315c19430fb4705716a5676a\n",
            "  Building wheel for cwl-eval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cwl-eval: filename=cwl_eval-1.0.10-py3-none-any.whl size=37797 sha256=59fe32576666e78e5bf481f465fafcd51dd82bc424ae47810cb9bec6b1e6d9bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/e9/ff/d2b6d72d9feb0d0b1b11aacfaf5cd866717034615c2d194093\n",
            "  Building wheel for cbor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cbor: filename=cbor-1.0.0-cp37-cp37m-linux_x86_64.whl size=51302 sha256=7781b065561a6098ff108e2235948e7a806d0e4ee1473a221c86919dc674cede\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/77/49/c9c2c8dc5848502e606e8579d0bbda18b850fb056a6c62239d\n",
            "  Building wheel for unlzw3 (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unlzw3: filename=unlzw3-0.2.1-py3-none-any.whl size=6082 sha256=1c9ed13808555d9a1e1570c218994f46dfa4f1f0503541bcf91f8a2766341ee1\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/3c/d9/e33962c8aad8999dc5560b6e71baafa2335c269f532f5e176a\n",
            "  Building wheel for warc3-wet-clueweb09 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for warc3-wet-clueweb09: filename=warc3_wet_clueweb09-0.2.5-py3-none-any.whl size=18922 sha256=65760f30ac0048e2ff1833e9b4e623a2e590c23349c0df97fa303b0bc6c9d641\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/d4/3c/7c2b0c3d400ad744e4db69f2fde166655da2ed2198bfc02db6\n",
            "  Building wheel for chest (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chest: filename=chest-0.2.3-py3-none-any.whl size=7634 sha256=cdc79cf547c0b9624fda8f19485a26218ebf2ab101344260988cdfb549913ba2\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/f5/b9/c436e11300809e6b40d46a5d2592fb0bff89e0712f2e878dc7\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1310 sha256=38b74af83caa0e3e1e12b0cd38617cad52d0f36d0017ada04488aed396c46eef\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=351d9fac39b0256b4e8a0f5ab746d3cf13ea92ea61f0239e77d044780fdc0dfd\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built python-terrier ir-measures cwl-eval cbor unlzw3 warc3-wet-clueweb09 chest sklearn wget\n",
            "Installing collected packages: cbor, zlib-state, warc3-wet-clueweb09, warc3-wet, unlzw3, typish, trec-car-tools, pytrec-eval-terrier, pyautocorpus, multiset, lz4, ijson, cwl-eval, wget, sklearn, pyjnius, nptyping, matchpy, ir-measures, ir-datasets, deprecation, chest, python-terrier\n",
            "Successfully installed cbor-1.0.0 chest-0.2.3 cwl-eval-1.0.10 deprecation-2.1.0 ijson-3.1.4 ir-datasets-0.5.3 ir-measures-0.3.1 lz4-4.0.2 matchpy-0.5.5 multiset-2.1.1 nptyping-1.4.4 pyautocorpus-0.1.8 pyjnius-1.3.0 python-terrier-0.8.1 pytrec-eval-terrier-0.5.4 sklearn-0.0 trec-car-tools-2.6 typish-1.9.3 unlzw3-0.2.1 warc3-wet-0.2.3 warc3-wet-clueweb09-0.2.5 wget-3.2 zlib-state-0.1.5\n"
          ]
        }
      ],
      "source": [
        "pip install python-terrier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzmhrfxwLzTc",
        "outputId": "ec15ba62-f0e3-406d-a8a8-e4cdd10eb218"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "terrier-assemblies 5.6 jar-with-dependencies not found, downloading to /root/.pyterrier...\n",
            "Done\n",
            "terrier-python-helper 0.0.6 jar not found, downloading to /root/.pyterrier...\n",
            "Done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTerrier 0.8.1 has loaded Terrier 5.6 (built by craigmacdonald on 2021-09-17 13:27)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pyterrier as pt\n",
        "pt.init()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJu99WTgL4fC",
        "outputId": "8c6085ac-f904-4d9a-f35d-f972e4cbe95c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usNLXkLhL_i3"
      },
      "outputs": [],
      "source": [
        "\n",
        "from collections import defaultdict\n",
        "import time\n",
        "\n",
        "import requests\n",
        "import ujson as json\n",
        "\n",
        "ERROR_KEY = u'error'\n",
        "STATUS_FIELD = u'status'\n",
        "\n",
        "URL_FIELD = u'url'\n",
        "PAGERANK_FIELD = u'pageRank'\n",
        "\n",
        "COSINE_FIELD = u'cosine'\n",
        "\n",
        "ANNOTATION_DATA_FIELD = u'annotation_data'\n",
        "\n",
        "_WIKIFIER_URL = u\"http://www.wikifier.org/annotate-article\"\n",
        "_WIKIFIER_MAX_SERVER_LIMIT = 25000\n",
        "WIKIFIER_MAX_CHAR_CEILING = round(_WIKIFIER_MAX_SERVER_LIMIT * .99)  # 99% of max allowed num chars for a post request\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "SENTENCE_AGGREGATOR = \" \"\n",
        "LEN_SENTENCE_AGGR = len(SENTENCE_AGGREGATOR)\n",
        "\n",
        "\n",
        "def _make_regex_with_escapes(escapers):\n",
        "    words_regex = r'{}[^_\\W]+{}'\n",
        "\n",
        "    temp_regexes = []\n",
        "    for escaper_pair in escapers:\n",
        "        (start, end) = escaper_pair\n",
        "        temp_regexes.append(words_regex.format(start, end))\n",
        "\n",
        "    return r\"|\".join(temp_regexes)\n",
        "\n",
        "\n",
        "def shallow_word_segment(phrase, escape_pairs=None):\n",
        "    \"\"\" Takes in a string phrase and segments it into words based on simple regex\n",
        "    Args:\n",
        "        phrase (str): phrase to be segmented to words\n",
        "        escape_pairs ([(str, str)]): list of tuples where each tuple is a pair of substrings that should not be\n",
        "                    used as word seperators. The motivation is to escapte special tags such as [HESITATION], ~SILENCE~\n",
        "                    IMPORTANT: Row regex has to be used when definng escapte pairs\n",
        "                        (\"[\", \"]\") will not work as [] are special chars in regex. Instead (\"\\[\", \"\\]\")\n",
        "    Returns:\n",
        "        ([str]): list of words extracted from the phrase\n",
        "    \"\"\"\n",
        "    if escape_pairs is None:\n",
        "        escape_pairs = []\n",
        "\n",
        "    escape_pairs.append((\"\", \"\"))\n",
        "\n",
        "    _regex = _make_regex_with_escapes(escape_pairs)\n",
        "    return re.findall(_regex, phrase, flags=re.UNICODE)\n",
        "\n",
        "\n",
        "def _segment_sentences(text):\n",
        "    \"\"\"segments a text into a set of sentences\n",
        "    Args:\n",
        "        text:\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "    import en_core_web_sm\n",
        "    nlp = en_core_web_sm.load()\n",
        "\n",
        "    text_sentences = nlp(text)\n",
        "\n",
        "    for sentence in text_sentences.sents:\n",
        "        yield sentence.text\n",
        "\n",
        "\n",
        "def partition_text(text, max_size):\n",
        "    \"\"\"takes a text string and creates a list of substrings that are shorter than a given length\n",
        "    Args:\n",
        "        text (str): text to be partitioned (usually a lecture transcript)\n",
        "        max_size (int): maximum number of characters one partition should contain\n",
        "    Returns:\n",
        "        chunks([str]): list of sub strings where each substring is shorter than the given length\n",
        "    \"\"\"\n",
        "    # get sentences\n",
        "    sentences = _segment_sentences(text)\n",
        "\n",
        "    chunks = []\n",
        "\n",
        "    temp_sents = []\n",
        "    temp_len = 0\n",
        "    for sentence in sentences:\n",
        "        len_sentence = len(sentence)\n",
        "        expected_len = temp_len + LEN_SENTENCE_AGGR + len_sentence  # estimate length cost\n",
        "        if expected_len > max_size:  # if it goes above threshold,\n",
        "            if len(temp_sents) > 0:\n",
        "                chunks.append(SENTENCE_AGGREGATOR.join(temp_sents))  # first load the preceding chunk\n",
        "                temp_sents = []\n",
        "                temp_len = 0\n",
        "\n",
        "        temp_sents.append(sentence)  # then aggregate the sentence to the temp chunk\n",
        "        temp_len += len_sentence\n",
        "\n",
        "    if len(temp_sents) > 0:\n",
        "        chunks.append(SENTENCE_AGGREGATOR.join(temp_sents))  # send the remainder chunk\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _get_wikififier_concepts(resp):\n",
        "    annotations = [{URL_FIELD: ann[URL_FIELD],\n",
        "                    COSINE_FIELD: ann[COSINE_FIELD],\n",
        "                    PAGERANK_FIELD: ann[PAGERANK_FIELD]} for ann in resp.get(\"annotations\", [])]\n",
        "\n",
        "    return {\n",
        "        ANNOTATION_DATA_FIELD: annotations,\n",
        "        STATUS_FIELD: resp[STATUS_FIELD]\n",
        "    }\n",
        "\n",
        "\n",
        "def _get_wikifier_response(text, api_key, df_ignore, words_ignore):\n",
        "    params = {\"text\": text,\n",
        "              \"userKey\": api_key,\n",
        "              \"nTopDfValuesToIgnore\": df_ignore,\n",
        "              \"nWordsToIgnoreFromList\": words_ignore}\n",
        "    r = requests.post(_WIKIFIER_URL, params)\n",
        "    if r.status_code == 200:\n",
        "        resp = json.loads(r.content)\n",
        "        if ERROR_KEY in resp:\n",
        "            raise ValueError(\"error in response : {}\".format(resp[ERROR_KEY]))\n",
        "        return resp\n",
        "    else:\n",
        "        raise ValueError(\"http status code 200 expected, got status code {} instead\".format(r.status_code))\n",
        "\n",
        "\n",
        "def wikify(text, key, df_ignore, words_ignore):\n",
        "    \"\"\"This function takes in a text representation of a lecture transcript and associates relevant Wikipedia topics to\n",
        "    it using www.wikifier.org entity linking technology.\n",
        "    Args:\n",
        "        text (str): text that needs to be Wikified (usually lecture transcript string)\n",
        "        key (str): API key for Wikifier obtained from http://www.wikifier.org/register.html\n",
        "        df_ignore (int): Most common words to ignore based on Document frequency\n",
        "        words_ignore (int): Most common words to ignore based on Term frequency\n",
        "    Returns:\n",
        "        [{key:val}]: a dict with status of the request and the list of Wiki topics linked to the text\n",
        "    \"\"\"\n",
        "    try:\n",
        "        resp = _get_wikifier_response(text, key, df_ignore, words_ignore)\n",
        "        resp[STATUS_FIELD] = 'success'\n",
        "    except ValueError as e:\n",
        "        try:\n",
        "            STATUS_ = e.message\n",
        "        except:\n",
        "            STATUS_ = e.args[0]\n",
        "        return {\n",
        "            STATUS_FIELD: STATUS_\n",
        "        }\n",
        "    time.sleep(0.5)\n",
        "    return _get_wikififier_concepts(resp)\n",
        "# values for Doc Frequency and Words to Ignore, more details about these variables\n",
        "# found at: http://www.wikifier.org/info.html\n",
        "DF_IGNORE_VAL = 50\n",
        "WORDS_IGNORE_VAL = 50\n",
        "\n",
        "\n",
        "def get_wikipedia_topic_features(text, api_key, chunk_size=5000):\n",
        "    \"\"\" get Wikification for the transcript using http://www.wikifier.org\n",
        "    Args:\n",
        "        text (str): text that needs to be Wikified\n",
        "        api_key (str): API key for Wikifier obtained from http://www.wikifier.org/register.html\n",
        "        chunk_size (int): maximum number of characters that need included in each Wikified fragment.\n",
        "    Returns:\n",
        "        enrichments ([{str: val}]): list of annotated chunks from the transcript\n",
        "    \"\"\"\n",
        "    text_partitions = partition_text(text, max_size=chunk_size)\n",
        "\n",
        "    enrichments = []\n",
        "    i = 1\n",
        "    for text_part in text_partitions:\n",
        "        temp_record = {}\n",
        "        annotations = wikify(text_part, api_key, DF_IGNORE_VAL, WORDS_IGNORE_VAL)\n",
        "        temp_record[\"part\"] = i\n",
        "        temp_record[\"text\"] = text_part\n",
        "        temp_record[\"annotations\"] = annotations\n",
        "        enrichments.append(temp_record)\n",
        "        i += 1\n",
        "\n",
        "    return enrichments\n",
        "\n",
        "\n",
        "def get_ranked_topics(chunks, option, top_n):\n",
        "    \"\"\" ranks the topics using the aggregated score across multiple Wikified chunks of the text.\n",
        "    Args:\n",
        "        chunks ([{str: val}]): list of Wikified chunks for the transcript\n",
        "        option {str}: pageRank or cosine\n",
        "        top_n (int): n top ranked topics of interest\n",
        "    Returns:\n",
        "        final_rec ({str:val}): dict with key for top_n_url or top_n_value and the URL or value of the topic\n",
        "    \"\"\"\n",
        "    chunks = list(chunks)\n",
        "\n",
        "    total_length = sum([len(part[\"text\"]) for part in chunks])\n",
        "\n",
        "    records = defaultdict(list)\n",
        "    for part in chunks:\n",
        "        annotations = part[\"annotations\"][\"annotation_data\"]\n",
        "        weight = len(part[\"text\"])\n",
        "        norm = weight / total_length\n",
        "        for concept in annotations:\n",
        "            url = concept[\"url\"]\n",
        "            val = concept.get(option, 0.)\n",
        "            records[url].append(val * norm)\n",
        "\n",
        "    rec = [(title, sum(val)) for title, val in records.items()]\n",
        "\n",
        "    # sort by normalised weight\n",
        "    rec.sort(key=lambda l: l[1], reverse=True)\n",
        "    n_recs = rec[:top_n]\n",
        "\n",
        "    final_rec = {}\n",
        "    for idx, item in enumerate(n_recs):\n",
        "        url, val = item\n",
        "        _idx = idx + 1\n",
        "        final_rec[\"topic_{}_{}_url\".format(_idx, option)] = url\n",
        "        final_rec[\"topic_{}_{}_val\".format(_idx, option)] = val\n",
        "\n",
        "    return final_rec\n",
        "\n",
        "\n",
        "def get_authority_wiki_features(text, api_key, top_n):\n",
        "    \"\"\" returns top-n most authoritative Wikipedia topics with PageRank scores.\n",
        "    Calculated using http://www.wikifier.org/\n",
        "    Args:\n",
        "        text (str): text that needs to be Wikified for authority\n",
        "        api_key (str): API key for Wikifier obtained from http://www.wikifier.org/register.html\n",
        "        top_n (int): n top ranking topics to be returned with PageRank scores\n",
        "    Returns:\n",
        "        ranked_topic_records ({str:val}): dict with key for top_n_url or top_n_value and the URL or value of the topic\n",
        "    \"\"\"\n",
        "    enriched_chunks = get_wikipedia_topic_features(text, api_key)\n",
        "    ranked_topic_records = get_ranked_topics(enriched_chunks, \"pageRank\", top_n)\n",
        "\n",
        "    return ranked_topic_records\n",
        "\n",
        "\n",
        "def get_coverage_wiki_features(text, api_key, top_n):\n",
        "    \"\"\" returns top-n most covered Wikipedia topics with cosine similarity scores.\n",
        "    Calculated using http://www.wikifier.org/\n",
        "    Args:\n",
        "        text (str): text that needs to be Wikified for coverage\n",
        "        api_key (str): API key for Wikifier obtained from http://www.wikifier.org/register.html\n",
        "        top_n (int): n top ranking topics to be returned with cosine scores\n",
        "    Returns:\n",
        "        ranked_topic_records ({str:val}): dict with key for top_n_url or top_n_value and the URL or value of the topic\n",
        "    \"\"\"\n",
        "    enriched_chunks = get_wikipedia_topic_features(text, api_key)\n",
        "    ranked_topic_records = get_ranked_topics(enriched_chunks, \"cosine\", top_n)\n",
        "\n",
        "    return ranked_topic_records\n",
        "\n",
        "\n",
        "\n",
        "import requests\n",
        "import spacy\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "\n",
        "import argparse\n",
        "import sys\n",
        "import string\n",
        "import spacy\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "\n",
        "\n",
        "def convert_string(hj):\n",
        "  new_s=[]\n",
        "  for token in hj:\n",
        "      new_s.append(str(token).strip(string.punctuation))\n",
        "  return ''.join(new_s)\n",
        "\n",
        "def get_concept_list(response):\n",
        "  concepts=[]\n",
        "  for i in range(len(response[\"annotation_data\"])):\n",
        "    url = response[\"annotation_data\"][i]['url']\n",
        "    cos = response[\"annotation_data\"][i]['cosine']\n",
        "    pr  = response[\"annotation_data\"][i]['pageRank']\n",
        "\n",
        "\n",
        "    concepts.append([convert_string(' '.join(url.split('/')[-1].split('_'))),cos,pr])\n",
        "  return concepts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZsNH0hhMqO_"
      },
      "outputs": [],
      "source": [
        "def get_wikipedia(doc_list):\n",
        "    key_api= \"\"\"boifszokpjjzgdxacixjqudgagqpgb\"\"\"\n",
        "    text=[]\n",
        "\n",
        "    for t in doc_list.text:\n",
        "      text.append(str(t))\n",
        "\n",
        "    gh=(''.join(text))\n",
        "    print(gh)\n",
        "    \n",
        "    get_response=wikify(gh,key_api,50,50)\n",
        "    converted_concepts=get_concept_list(get_response)\n",
        "    print(converted_concepts)\n",
        "    return converted_concepts\n",
        "\n",
        "\n",
        "\n",
        "def update_xml(xml, nouns, f_name):\n",
        "    n_queries = 0\n",
        "\n",
        "    tree = ET.parse(xml)\n",
        "    for node in tree.iter(): \n",
        "        if node.tag == 'query':\n",
        "            n_queries += 1\n",
        "            print(node.text)  \n",
        "            gather = ' '.join(list(set(nouns[n_queries-1])))\n",
        "            tmp = node.text + ' ' + gather\n",
        "            print(tmp)\n",
        "            tmp = tmp.lower()\n",
        "            node.text = tmp\n",
        "    if n_queries != len(nouns):\n",
        "        print('Warning: n_queries do not match with number '\n",
        "                'of descriptions parsed', file=sys.stderr)\n",
        "\n",
        "    tree.write(f_name)\n",
        "\n",
        "\n",
        "\n",
        "top=\"/content/drive/MyDrive/IRDM/ent_test.xml\"\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "tree = ET.parse(top)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lnd57wHgQIGs"
      },
      "outputs": [],
      "source": [
        "# Create a query dict \n",
        "\n",
        "queries_dict={}\n",
        "for node in tree.iter():\n",
        "  if node.tag=='num':\n",
        "    key=node.text\n",
        "  if node.tag==\"query\":\n",
        "        queries_dict.update({key:get_concept_list(wikify(node.text,'boifszokpjjzgdxacixjqudgagqpgb',50,50))})\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IVsB7rZRmFV"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bs2qSEflR9bi"
      },
      "outputs": [],
      "source": [
        "np.save('/content/drive/MyDrive/IRDM/queries_doc_test3.npy', queries_dict) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kltNxz38R7-P"
      },
      "outputs": [],
      "source": [
        "# Reading all the batch file locations to merge them \n",
        "import os\n",
        "iter=[]\n",
        "name_dir=\"/content/drive/MyDrive/IRDM/\"\n",
        "for root, folder, data in os.walk(name_dir):\n",
        "    for a in data:\n",
        "        if \".npy\" in a:\n",
        "          if \"batch\" in a:\n",
        "            iter.append((os.path.join(root,a)))\n",
        "          "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KilRc7QEYHaF"
      },
      "outputs": [],
      "source": [
        "# Merger function\n",
        "def mergmast(list_of_dicts):\n",
        "    master={}\n",
        "    for i in range(len(list_of_dicts)):\n",
        "      open = np.load(list_of_dicts[i],allow_pickle='TRUE').item()\n",
        "      master.update(open)\n",
        "    return master"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "J5IpTYrC1wFY",
        "outputId": "5c8c07f8-7ea2-4474-a237-040b0a12ba45"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/IRDM/batch_o.npy'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Delete the test batch\n",
        "iter.remove(\"/content/drive/MyDrive/IRDM/batch_o.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BtbW7k6J1Ayb"
      },
      "outputs": [],
      "source": [
        "#Generating the entire batch\n",
        "total_batch_156k=mergmast(iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dITCgVIz1XED",
        "outputId": "8c6a8df6-23b5-4ddf-bfbb-17e6c8f5bffd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150270"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "len(total_batch_156k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6W4GUh957ak"
      },
      "outputs": [],
      "source": [
        "# Saving the entire batch\n",
        "np.save('/content/drive/MyDrive/IRDM/master_set_docs.npy', total_batch_156k) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DXlfc39bTYb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}