{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Wikipedia Code Referenced from https://github.com/sahanbull/context-agnostic-engagement/blob/0472e76c6bd00d686b235d844e2fb4d71649400c/context_agnostic_engagement/feature_extraction/_api_utils.py#L47"
      ],
      "metadata": {
        "id": "jhowcRA86c7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "jCFP5o216SXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fofPVdlQnHLH",
        "outputId": "252defba-f3d3-4532-ff4b-92dd13e14d66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IElN5NEWocOy"
      },
      "outputs": [],
      "source": [
        "# Text Files Generator \n",
        "import numpy as np\n",
        "master_dict = np.load('/content/drive/MyDrive/IRDM/files_rel.npy',allow_pickle='TRUE').item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vqStKV9ohCy"
      },
      "outputs": [],
      "source": [
        "# Creation of batches and the text file is read above\n",
        "# You can split the data in batches and run the wikification below in batch saving \n",
        "\n",
        "batch_1k_1=list(master_dict.keys())[130000:131000]\n",
        "batch_1k_2= list(master_dict.keys())[131000:132000]\n",
        "batch_1k_3= list(master_dict.keys())[132000:133000]\n",
        "batch_1k_4= list(master_dict.keys())[133000:134000]\n",
        "batch_1k_5= list(master_dict.keys())[134000:135000]\n",
        "batch_1k_6= list(master_dict.keys())[135000:136000]\n",
        "batch_1k_7= list(master_dict.keys())[136000:137000]\n",
        "batch_1k_8= list(master_dict.keys())[137000:128000]\n",
        "batch_1k_9= list(master_dict.keys())[138000:139000]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrRrGQmUyMWi",
        "outputId": "f2fc6ab4-4e9e-455f-b852-95756e8a975a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "138605"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(master_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omCxUa3fW05n",
        "outputId": "ec2109c4-ef88-4c8a-a91d-9808f6e5d0fb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "14295"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(master_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86DhFUEl5J5v"
      },
      "outputs": [],
      "source": [
        "np.save('/content/drive/MyDrive/IRDM/files_rel_train.npy', master_dict) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0UWr5085m5q"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import time\n",
        "\n",
        "import requests\n",
        "import ujson as json\n",
        "\n",
        "ERROR_KEY = u'error'\n",
        "STATUS_FIELD = u'status'\n",
        "\n",
        "URL_FIELD = u'url'\n",
        "PAGERANK_FIELD = u'pageRank'\n",
        "\n",
        "COSINE_FIELD = u'cosine'\n",
        "\n",
        "ANNOTATION_DATA_FIELD = u'annotation_data'\n",
        "\n",
        "_WIKIFIER_URL = u\"http://www.wikifier.org/annotate-article\"\n",
        "_WIKIFIER_MAX_SERVER_LIMIT = 25000\n",
        "WIKIFIER_MAX_CHAR_CEILING = round(_WIKIFIER_MAX_SERVER_LIMIT * .99)  # 99% of max allowed num chars for a post request\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "SENTENCE_AGGREGATOR = \" \"\n",
        "LEN_SENTENCE_AGGR = len(SENTENCE_AGGREGATOR)\n",
        "\n",
        "\n",
        "\n",
        "import re\n",
        "import requests\n",
        "\n",
        "def get_page_tag(url, title_re=re.compile(r'<title>(.*?)</title>', re.UNICODE )):\n",
        "    \n",
        "    \"\"\"\n",
        "    Retrieves the title tag from a given url (or actually any tag if you want..)\n",
        "    \n",
        "    Requirements:\n",
        "    requests (pip install requests)\n",
        "    \n",
        "    Example usage:\n",
        "    \n",
        "    Simple\n",
        "        >>> title = get_page_tag(\"http://www.tv2.no/nyheter/innenriks/her-ligger-24-tonn-kylling-i-groefta-3692748.html\")\n",
        "    \n",
        "    Supply your own regular expression to filter out what you want to be returned\n",
        "    \n",
        "        >>> myre = re.compile(r'<title>(.*?) - TV 2 Nyhetene</title>', re.UNICODE\n",
        "        >>> title = get_page_tag(\"http://www.tv2.no/nyheter/innenriks/her-ligger-24-tonn-kylling-i-groefta-3692748.html\", title_re=myre)\n",
        "        \n",
        "    \"\"\"\n",
        "\n",
        "    r = requests.get(url)\n",
        "    if r.status_code == 200:\n",
        "        match = title_re.search(r.text)\n",
        "        if match:\n",
        "            return match.group(1)\n",
        "        return Exception(\"No match for title in page\")\n",
        "    raise Exception(r.status_code)\n",
        "\n",
        "\n",
        "def _make_regex_with_escapes(escapers):\n",
        "    words_regex = r'{}[^_\\W]+{}'\n",
        "\n",
        "    temp_regexes = []\n",
        "    for escaper_pair in escapers:\n",
        "        (start, end) = escaper_pair\n",
        "        temp_regexes.append(words_regex.format(start, end))\n",
        "\n",
        "    return r\"|\".join(temp_regexes)\n",
        "\n",
        "\n",
        "def shallow_word_segment(phrase, escape_pairs=None):\n",
        "    \"\"\" Takes in a string phrase and segments it into words based on simple regex\n",
        "    Args:\n",
        "        phrase (str): phrase to be segmented to words\n",
        "        escape_pairs ([(str, str)]): list of tuples where each tuple is a pair of substrings that should not be\n",
        "                    used as word seperators. The motivation is to escapte special tags such as [HESITATION], ~SILENCE~\n",
        "                    IMPORTANT: Row regex has to be used when definng escapte pairs\n",
        "                        (\"[\", \"]\") will not work as [] are special chars in regex. Instead (\"\\[\", \"\\]\")\n",
        "    Returns:\n",
        "        ([str]): list of words extracted from the phrase\n",
        "    \"\"\"\n",
        "    if escape_pairs is None:\n",
        "        escape_pairs = []\n",
        "\n",
        "    escape_pairs.append((\"\", \"\"))\n",
        "\n",
        "    _regex = _make_regex_with_escapes(escape_pairs)\n",
        "    return re.findall(_regex, phrase, flags=re.UNICODE)\n",
        "\n",
        "\n",
        "def _segment_sentences(text):\n",
        "    \"\"\"segments a text into a set of sentences\n",
        "    Args:\n",
        "        text:\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "    import en_core_web_sm\n",
        "    nlp = en_core_web_sm.load()\n",
        "\n",
        "    text_sentences = nlp(text)\n",
        "\n",
        "    for sentence in text_sentences.sents:\n",
        "        yield sentence.text\n",
        "\n",
        "\n",
        "def partition_text(text, max_size):\n",
        "    \"\"\"takes a text string and creates a list of substrings that are shorter than a given length\n",
        "    Args:\n",
        "        text (str): text to be partitioned (usually a lecture transcript)\n",
        "        max_size (int): maximum number of characters one partition should contain\n",
        "    Returns:\n",
        "        chunks([str]): list of sub strings where each substring is shorter than the given length\n",
        "    \"\"\"\n",
        "    # get sentences\n",
        "    sentences = _segment_sentences(text)\n",
        "\n",
        "    chunks = []\n",
        "\n",
        "    temp_sents = []\n",
        "    temp_len = 0\n",
        "    for sentence in sentences:\n",
        "        len_sentence = len(sentence)\n",
        "        expected_len = temp_len + LEN_SENTENCE_AGGR + len_sentence  # estimate length cost\n",
        "        if expected_len > max_size:  # if it goes above threshold,\n",
        "            if len(temp_sents) > 0:\n",
        "                chunks.append(SENTENCE_AGGREGATOR.join(temp_sents))  # first load the preceding chunk\n",
        "                temp_sents = []\n",
        "                temp_len = 0\n",
        "\n",
        "        temp_sents.append(sentence)  # then aggregate the sentence to the temp chunk\n",
        "        temp_len += len_sentence\n",
        "\n",
        "    if len(temp_sents) > 0:\n",
        "        chunks.append(SENTENCE_AGGREGATOR.join(temp_sents))  # send the remainder chunk\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _get_wikififier_concepts(resp):\n",
        "    annotations = [{URL_FIELD: ann[URL_FIELD],\n",
        "                    COSINE_FIELD: ann[COSINE_FIELD],\n",
        "                    PAGERANK_FIELD: ann[PAGERANK_FIELD]} for ann in resp.get(\"annotations\", [])]\n",
        "\n",
        "    return {\n",
        "        ANNOTATION_DATA_FIELD: annotations,\n",
        "        STATUS_FIELD: resp[STATUS_FIELD]\n",
        "    }\n",
        "\n",
        "\n",
        "def _get_wikifier_response(text, api_key, df_ignore, words_ignore):\n",
        "    params = {\"text\": text,\n",
        "              \"userKey\": api_key,\n",
        "              \"nTopDfValuesToIgnore\": df_ignore,\n",
        "              \"nWordsToIgnoreFromList\": words_ignore}\n",
        "    r = requests.post(_WIKIFIER_URL, params)\n",
        "    if r.status_code == 200:\n",
        "        resp = json.loads(r.content)\n",
        "        if ERROR_KEY in resp:\n",
        "            raise ValueError(\"error in response : {}\".format(resp[ERROR_KEY]))\n",
        "        return resp\n",
        "    else:\n",
        "        raise ValueError(\"http status code 200 expected, got status code {} instead\".format(r.status_code))\n",
        "\n",
        "\n",
        "def wikify(text, key, df_ignore, words_ignore):\n",
        "    \"\"\"This function takes in a text representation of a lecture transcript and associates relevant Wikipedia topics to\n",
        "    it using www.wikifier.org entity linking technology.\n",
        "    Args:\n",
        "        text (str): text that needs to be Wikified (usually lecture transcript string)\n",
        "        key (str): API key for Wikifier obtained from http://www.wikifier.org/register.html\n",
        "        df_ignore (int): Most common words to ignore based on Document frequency\n",
        "        words_ignore (int): Most common words to ignore based on Term frequency\n",
        "    Returns:\n",
        "        [{key:val}]: a dict with status of the request and the list of Wiki topics linked to the text\n",
        "    \"\"\"\n",
        "    try:\n",
        "        resp = _get_wikifier_response(text, key, df_ignore, words_ignore)\n",
        "        resp[STATUS_FIELD] = 'success'\n",
        "    except ValueError as e:\n",
        "        try:\n",
        "            STATUS_ = e.message\n",
        "        except:\n",
        "            STATUS_ = e.args[0]\n",
        "        return {\n",
        "            STATUS_FIELD: STATUS_\n",
        "        }\n",
        "    time.sleep(0.5)\n",
        "    return _get_wikififier_concepts(resp)\n",
        "# values for Doc Frequency and Words to Ignore, more details about these variables\n",
        "# found at: http://www.wikifier.org/info.html\n",
        "DF_IGNORE_VAL = 50\n",
        "WORDS_IGNORE_VAL = 50\n",
        "\n",
        "\n",
        "def get_wikipedia_topic_features(text, api_key, chunk_size=5000):\n",
        "    \"\"\" get Wikification for the transcript using http://www.wikifier.org\n",
        "    Args:\n",
        "        text (str): text that needs to be Wikified\n",
        "        api_key (str): API key for Wikifier obtained from http://www.wikifier.org/register.html\n",
        "        chunk_size (int): maximum number of characters that need included in each Wikified fragment.\n",
        "    Returns:\n",
        "        enrichments ([{str: val}]): list of annotated chunks from the transcript\n",
        "    \"\"\"\n",
        "    text_partitions = partition_text(text, max_size=chunk_size)\n",
        "\n",
        "    enrichments = []\n",
        "    i = 1\n",
        "    for text_part in text_partitions:\n",
        "        temp_record = {}\n",
        "        annotations = wikify(text_part, api_key, DF_IGNORE_VAL, WORDS_IGNORE_VAL)\n",
        "        temp_record[\"part\"] = i\n",
        "        temp_record[\"text\"] = text_part\n",
        "        temp_record[\"annotations\"] = annotations\n",
        "        enrichments.append(temp_record)\n",
        "        i += 1\n",
        "\n",
        "    return enrichments\n",
        "\n",
        "\n",
        "def get_ranked_topics(chunks, option, top_n):\n",
        "    \"\"\" ranks the topics using the aggregated score across multiple Wikified chunks of the text.\n",
        "    Args:\n",
        "        chunks ([{str: val}]): list of Wikified chunks for the transcript\n",
        "        option {str}: pageRank or cosine\n",
        "        top_n (int): n top ranked topics of interest\n",
        "    Returns:\n",
        "        final_rec ({str:val}): dict with key for top_n_url or top_n_value and the URL or value of the topic\n",
        "    \"\"\"\n",
        "    chunks = list(chunks)\n",
        "\n",
        "    total_length = sum([len(part[\"text\"]) for part in chunks])\n",
        "\n",
        "    records = defaultdict(list)\n",
        "    for part in chunks:\n",
        "        annotations = part[\"annotations\"][\"annotation_data\"]\n",
        "        weight = len(part[\"text\"])\n",
        "        norm = weight / total_length\n",
        "        for concept in annotations:\n",
        "            url = concept[\"url\"]\n",
        "            val = concept.get(option, 0.)\n",
        "            records[url].append(val * norm)\n",
        "\n",
        "    rec = [(title, sum(val)) for title, val in records.items()]\n",
        "\n",
        "    # sort by normalised weight\n",
        "    rec.sort(key=lambda l: l[1], reverse=True)\n",
        "    n_recs = rec[:top_n]\n",
        "\n",
        "    final_rec = {}\n",
        "    for idx, item in enumerate(n_recs):\n",
        "        url, val = item\n",
        "        _idx = idx + 1\n",
        "        final_rec[\"topic_{}_{}_url\".format(_idx, option)] = url\n",
        "        final_rec[\"topic_{}_{}_val\".format(_idx, option)] = val\n",
        "\n",
        "    return final_rec\n",
        "\n",
        "\n",
        "def get_authority_wiki_features(text, api_key, top_n):\n",
        "    \"\"\" returns top-n most authoritative Wikipedia topics with PageRank scores.\n",
        "    Calculated using http://www.wikifier.org/\n",
        "    Args:\n",
        "        text (str): text that needs to be Wikified for authority\n",
        "        api_key (str): API key for Wikifier obtained from http://www.wikifier.org/register.html\n",
        "        top_n (int): n top ranking topics to be returned with PageRank scores\n",
        "    Returns:\n",
        "        ranked_topic_records ({str:val}): dict with key for top_n_url or top_n_value and the URL or value of the topic\n",
        "    \"\"\"\n",
        "    enriched_chunks = get_wikipedia_topic_features(text, api_key)\n",
        "    ranked_topic_records = get_ranked_topics(enriched_chunks, \"pageRank\", top_n)\n",
        "\n",
        "    return ranked_topic_records\n",
        "\n",
        "\n",
        "def get_coverage_wiki_features(text, api_key, top_n):\n",
        "    \"\"\" returns top-n most covered Wikipedia topics with cosine similarity scores.\n",
        "    Calculated using http://www.wikifier.org/\n",
        "    Args:\n",
        "        text (str): text that needs to be Wikified for coverage\n",
        "        api_key (str): API key for Wikifier obtained from http://www.wikifier.org/register.html\n",
        "        top_n (int): n top ranking topics to be returned with cosine scores\n",
        "    Returns:\n",
        "        ranked_topic_records ({str:val}): dict with key for top_n_url or top_n_value and the URL or value of the topic\n",
        "    \"\"\"\n",
        "    enriched_chunks = get_wikipedia_topic_features(text, api_key)\n",
        "    ranked_topic_records = get_ranked_topics(enriched_chunks, \"cosine\", top_n)\n",
        "\n",
        "    return ranked_topic_records\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8fyg6ls7SjQ"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import spacy\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "\n",
        "import argparse\n",
        "import sys\n",
        "import string\n",
        "import spacy\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "\n",
        "def convert_string(hj):\n",
        "  new_s=[]\n",
        "  for token in hj:\n",
        "      new_s.append(str(token).strip(string.punctuation))\n",
        "  return ''.join(new_s)\n",
        "\n",
        "def get_concept_list(response):\n",
        "  concepts=[]\n",
        "  for i in range(len(response[\"annotation_data\"])):\n",
        "    url = response[\"annotation_data\"][i]['url']\n",
        "    cos = response[\"annotation_data\"][i]['cosine']\n",
        "    pr  = response[\"annotation_data\"][i]['pageRank']\n",
        "\n",
        "\n",
        "    concepts.append([convert_string(' '.join(url.split('/')[-1].split('_'))),cos,pr])\n",
        "  return concepts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYWAev5V7YC9"
      },
      "outputs": [],
      "source": [
        "tab_key= \"\"\"qgllexjwcwghzerkvodpdawsfsiexg\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cGfyzhQ9FXU"
      },
      "outputs": [],
      "source": [
        "sample_input= \"\"\"Set the crib man smoking at Asian fantasy dragging them teacakes thing about how beautiful life is man. Check it out smoke em, if you got em came to the top all the way from the bottom and I'm just trying to count my blessings. I wouldn't leave you with. All right friends, very smooth intro right there. Shout out to fir sellout the fern. I'm the mic Mike's weeks with no episode 16 apologies already blowing it very special podcast episode or bad depending on if you like the kid or not. We got our buddy. That's a debate into at boom Shenanigans, right? There's there dot between that seems underscore right? That's right. That's what everyone. Yeah. We got her buddy Sammy. He's an actor comedian and most importantly if you fall. Me on Instagram our CEO of our fantasy football team. That's your value to me at least a me on my team by the way. Yeah. I think he's rolling you guys I think so. Yeah, I could roll. Yes. It's going on nice going on man what they know and he stopped and had a show in Encinitas. He has a show this Saturday in Clermont at the comedy Palace nice. He has another one in Vista on Friday, October 25th coming up. So couple shows you here's my biggest group either. I give you disclose that the more that I know that I forgot was like, you know always 8 minutes 10 minutes and\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvBw8S5D8B9I"
      },
      "outputs": [],
      "source": [
        "for i in  range(10):\n",
        "  get_response=wikify(sample_input,tab_key,50,50)\n",
        "  converted_concepts=get_concept_list(get_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcddEC0UQMX7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkFWU8H3EvWZ"
      },
      "outputs": [],
      "source": [
        "def shadowfax(dict_with_files,mapper):\n",
        "  doc_concept={}\n",
        "  for i in range(len(mapper)):\n",
        "    key=mapper[i]\n",
        "    doc_concept.update({key:get_concept_list(wikify(dict_with_files[key],tab_key,50,50))})\n",
        "  return doc_concept\n",
        "\n",
        "\n",
        "batch_one=shadowfax(master_dict,batch_1k_1)\n",
        "np.save('/content/drive/MyDrive/IRDM/batch_tk_131.npy', batch_one) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugJyo-2-a83y"
      },
      "outputs": [],
      "source": [
        "def shadowfax(dict_with_files,mapper):\n",
        "  doc_concept={}\n",
        "  for i in range(len(mapper)):\n",
        "    key=mapper[i]\n",
        "    doc_concept.update({key:get_concept_list(wikify(dict_with_files[key],tab_key,50,50))})\n",
        "  return doc_concept\n",
        "\n",
        "\n",
        "batch_one=shadowfax(master_dict,batch_1k_2)\n",
        "np.save('/content/drive/MyDrive/IRDM/batch_tk_132.npy', batch_one) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qCU-bB2Pe4MK"
      },
      "outputs": [],
      "source": [
        "def shadowfax(dict_with_files,mapper):\n",
        "  doc_concept={}\n",
        "  for i in range(len(mapper)):\n",
        "    key=mapper[i]\n",
        "    doc_concept.update({key:get_concept_list(wikify(dict_with_files[key],tab_key,50,50))})\n",
        "  return doc_concept\n",
        "\n",
        "\n",
        "batch_one=shadowfax(master_dict,batch_1k_3)\n",
        "np.save('/content/drive/MyDrive/IRDM/batch_tk_133.npy', batch_one) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Q32swXHXe92U"
      },
      "outputs": [],
      "source": [
        "def shadowfax(dict_with_files,mapper):\n",
        "  doc_concept={}\n",
        "  for i in range(len(mapper)):\n",
        "    key=mapper[i]\n",
        "    doc_concept.update({key:get_concept_list(wikify(dict_with_files[key],tab_key,50,50))})\n",
        "  return doc_concept\n",
        "\n",
        "\n",
        "batch_one=shadowfax(master_dict,batch_1k_4)\n",
        "np.save('/content/drive/MyDrive/IRDM/batch_tk_314.npy', batch_one) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MHZ7J1i-fDqU"
      },
      "outputs": [],
      "source": [
        "def shadowfax(dict_with_files,mapper):\n",
        "  doc_concept={}\n",
        "  for i in range(len(mapper)):\n",
        "    key=mapper[i]\n",
        "    doc_concept.update({key:get_concept_list(wikify(dict_with_files[key],tab_key,50,50))})\n",
        "  return doc_concept\n",
        "\n",
        "\n",
        "batch_one=shadowfax(master_dict,batch_1k_5)\n",
        "np.save('/content/drive/MyDrive/IRDM/batch_tk_135.npy', batch_one) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nCQbLJtXfLNn"
      },
      "outputs": [],
      "source": [
        "def shadowfax(dict_with_files,mapper):\n",
        "  doc_concept={}\n",
        "  for i in range(len(mapper)):\n",
        "    key=mapper[i]\n",
        "    doc_concept.update({key:get_concept_list(wikify(dict_with_files[key],tab_key,50,50))})\n",
        "  return doc_concept\n",
        "\n",
        "\n",
        "batch_one=shadowfax(master_dict,batch_1k_6)\n",
        "np.save('/content/drive/MyDrive/IRDM/batch_tk_136.npy', batch_one) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ikbKEDDLfPB5"
      },
      "outputs": [],
      "source": [
        "def shadowfax(dict_with_files,mapper):\n",
        "  doc_concept={}\n",
        "  for i in range(len(mapper)):\n",
        "    key=mapper[i]\n",
        "    doc_concept.update({key:get_concept_list(wikify(dict_with_files[key],tab_key,50,50))})\n",
        "  return doc_concept\n",
        "\n",
        "\n",
        "batch_one=shadowfax(master_dict,batch_1k_7)\n",
        "np.save('/content/drive/MyDrive/IRDM/batch_tk_137.npy', batch_one) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nPDXBAJVfTq1"
      },
      "outputs": [],
      "source": [
        "def shadowfax(dict_with_files,mapper):\n",
        "  doc_concept={}\n",
        "  for i in range(len(mapper)):\n",
        "    key=mapper[i]\n",
        "    doc_concept.update({key:get_concept_list(wikify(dict_with_files[key],tab_key,50,50))})\n",
        "  return doc_concept\n",
        "\n",
        "\n",
        "batch_one=shadowfax(master_dict,batch_1k_8)\n",
        "np.save('/content/drive/MyDrive/IRDM/batch_tk_138.npy', batch_one) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SVMbXpVtlz0h"
      },
      "outputs": [],
      "source": [
        "def shadowfax(dict_with_files,mapper):\n",
        "  doc_concept={}\n",
        "  for i in range(len(mapper)):\n",
        "    key=mapper[i]\n",
        "    doc_concept.update({key:get_concept_list(wikify(dict_with_files[key],tab_key,50,50))})\n",
        "  return doc_concept\n",
        "\n",
        "\n",
        "batch_one=shadowfax(master_dict,batch_1k_9)\n",
        "np.save('/content/drive/MyDrive/IRDM/batch_tk_139.npy', batch_one) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6baTps5l8I8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FRR6b-nAl3k7"
      },
      "outputs": [],
      "source": [
        "def shadowfax(dict_with_files,mapper):\n",
        "  doc_concept={}\n",
        "  for i in range(len(mapper)):\n",
        "    key=mapper[i]\n",
        "    doc_concept.update({key:get_concept_list(wikify(dict_with_files[key],tab_key,50,50))})\n",
        "  return doc_concept\n",
        "\n",
        "\n",
        "batch_one=shadowfax(master_dict,batch_1k_10)\n",
        "np.save('/content/drive/MyDrive/IRDM/batch_tk_120.npy', batch_one) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjGFK6xbl7gB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}